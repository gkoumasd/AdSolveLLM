<section id="ws" class="services">
  <!-- WS1 -->
  <div class="container" data-aos="fade-up">

    <div class="icon-box1">
      <div id="ws1" class="icon"><i class="ri-shake-hands-line"></i></div>
      <h4>Work Stream 1: Co-production and Criteria for Responsible Research and Innovation (RRI) with LLMs</h4>

      <p><b>Background & challenges:</b> Regulatory frameworks, guidance and proposed legislation have been introduced
        to address privacy, accuracy and explainability of automated systems (EU AI Act, UK Government’s National AI
        Strategy, UK Algorithmic Transparency, UK Government National Data Strategy, UK White Paper: A Pro-innovation
        Approach to AI Regulation, UK ICO guidance on explaining AI). It is not clear how domain experts can translate
        this regulation to meet needs for LLM technology use.</p>
      <p><b>Foci & implementation:</b> We will review and synthesise regulatory expectations and principles of ethical
        and responsible innovation into a set of requirement criteria for safe and responsible use of AI within medicine
        and the law. Such criteria are expected to include e.g. privacy preservation, inclusivity, factuality,
        situational awareness, explicability. The full set of criteria will be defined through interactions within our
        multi-disciplinary team, partners and relevant stakeholders, guided by the RRI AREA framework, and in the light
        of ethics literature that goes beyond the AREA framework. We will examine how the criteria translate to
        practical needs in measuring safety and effectiveness in law and healthcare, especially through scoping
        workshops with partners. A co-creation ‘by design’ approach will be key at this stage to shape the lifecycle of
        LLM-based legal and medical AI. Specifically the methods in this WP will involve: (a) surveying regulatory
        frameworks, the literature on ethical challenges and safety issues in the application of technology in legal
        practice and medical diagnostics as well as suitable analysis and design frameworks; (b) consultations,
        interviews, studies of work practices through observation and discussions with practitioners; (c) scoping
        workshops with partners and all co-Is, lead by the RAi and domain experts to establish requirement criteria that
        translate frameworks into practice in the context of legal and medical use cases. The workshops will help
        further refine the use cases; (d) A series of scenario-based co-creation workshops where use case prototypes
        will be progressively evolved and evaluated by stakeholders using benchmarks developed in Work Stream 2.</p>
      <p><b>Outputs:</b> We will obtain <b>requirements criteria & associated publications for RRI with LLMs</b> for
        real-world applications within the identified high stake medical and legal use cases. This Work Stream will
        drive a <b>co-production feedback loop with stakeholders</b>.</p>
    </div>
  </div>

  <br>

  <!-- WS2 -->
  <div class="container" data-aos="fade-up">
    <div class="icon-box1">
      <div id="ws2" class="icon"><i class="ri-bar-chart-box-line"></i></div>
      <h4>Work Stream 2: Evaluation Benchmark for RRI</h4>

      <p><b>Background & challenges:</b> State-of-the-art evaluation of LLMs has resulted in large benchmarks containing
        hundreds of linguistic and programmatic tasks (e.g. reasoning, mathematics, context free QA) and associated
        metrics meant to assess LLM capabilities. While such benchmarks offer a more challenging evaluation setting for
        AI systems, requiring more than memorisation ability for many of the tasks, they tell us nothing about the
        real-world applicability of LLMs in social applications. Evaluation of GPT-4 on medical QA has shown impressive
        results, with GPT-4 performing 20 points above average on samples from the U.S. Medical Licensing Examination.
        However, solving exam multiple-choice questions is far removed from the ability to diagnose or monitor a patient
        over time. </p>
      <p><b>Foci & implementation:</b> We will operationalise satisfiability of the RRI criteria identified in Work
        Stream 1 by implementing novel metrics and tasks developed through co-creation. Where appropriate strategies
        (e.g. memorisation prompting), metrics, tasks and data from existing benchmarks will be included (e.g.
        counterfactual tasks, inference-based metrics from big-bench or HELM) with the addition of new metrics (e.g. for
        privacy, inclusivity) and a focus on medical and legal needs. For example for privacy, we will explore
        re-identification tasks, and adversarial scenarios; for health monitoring, e.g. rationale-based classification,
        measured against known phenotypes; medical summarisation evaluation against evidence and human preferences using
        inference based and other metrics. An important consideration will be around how to protect evaluation
        benchmarks from leaking into training data; according to the recent statement to the Lords by OpenAI, the
        evaluation of LLMs needs to be performed by independent assessors. We will also explore scenarios for
        independent model assessment and safe sharing of datasets for training purposes.</p>
      <p><b>Outputs:</b> <b>Evaluation framework and benchmarks for RRI with LLMs</b>, which will drive and evaluate the
        project research, drive the feed-back loop between domain and technical experts, and ultimately provide an
        evaluation landmark for LLMs globally.</p>
    </div>
  </div>

  <br>

  <!-- WS3 -->
  <div class="container" data-aos="fade-up">
    <div class="icon-box1">
      <div id="ws3" class="icon"><i class="ri-flow-chart"></i></div>
      <h4>Work Stream 3: Temporal Reasoning and Situational Awareness</h4>
      <p><b>Background & challenges:</b> Most social applications, be it summarisation of long documents (e.g. court
        cases, therapy sessions) or dialogue, require contextual awareness, both in terms of temporal and situational
        reasoning, shown to be lacking in LLMs. Specifically LLMs lag behind human performance and even small-scale
        specialised models on tasks such as temporal ordering of timelines, satisfaction of temporal constraints (before
        and after) and common sense knowledge about events. </p>
      <p><b>Foci & implementation:</b> To mitigate shortcomings in temporal and situational awareness we will
        investigate instruction fine-tuning for temporal relations and use-case relevant situations informed by domain
        experts and responsible AI principles. We will also investigate: personalisation strategies during both
        pre-training and fine-tuning; hybrid architectures combining & augmenting LLMs with smaller specialised models
        and symbolic reasoning, the injection of temporal information into neural architectures, including both
        transformers and novel architectures better at handling sequential data. The effectiveness of developed methods
        for improving temporal reasoning will be evaluated intrinsically against known benchmarks, and the benchmark
        from Work Stream 2, and extrinsically in terms of their impact on summary quality for the legal and mental
        health use cases. The temporal reasoning & situational awareness methods will also feed into and be evaluated in
        the dialogue (Work Stream 4) and monitoring tasks (Work Stream 5).</p>
      <p><b>Outputs:</b> We will develop world-leading pioneering <b>methods, code, publications for augmenting temporal
          &
          situational reasoning in LLMs</b> to be used by researchers in industry and academia alike. These will enable
        time
        and situation aware summarisation and dialogue prototypes.</p>
    </div>
  </div>

  <br>

  <!-- WS4 -->
  <div class="container" data-aos="fade-up">
    <div class="icon-box1">
      <div id="ws4" class="icon"><i class="ri-question-answer-line"></i></div>
      <h4>Work Stream 4: Interactions</h4>
      <div class="header" id="ws5b">
        <h5>Generation and dialogue technology for sensitive data</h5>
      </div>
      <p><b>Background & challenges:</b> Dialogue systems are the leading public facing AI technology. Yet their
        suitability for use with sensitive data or vulnerable individuals is questionable. Issues include privacy leaks,
        harmful biases, inappropriate responses, poor situational reasoning and hallucinations. These constitute
        limitations that hinder appropriate generation (correct, safe and suitable), relevant to both summarisation and
        dialogue needs within use cases. </p>
      <p><b>Foci & implementation: </b>We will investigate methods of addressing such limitations to enable: (a) safe
        and useful summarisation of court cases, mental health therapy sessions & social media timelines and (b)
        appropriate generation of responses in dialogue systems for legal advice (e.g. with simplified terminology for
        lay users) and AI supported self-management. For privacy, author obfuscation methods including diversification
        in paraphrasing and training data will be explored. For situational reasoning, methods from Work Stream 3 will
        be included and evaluated in summarisation and dialogue. To prevent hallucinations, we will investigate
        generation using a blueprint for both summarisation and dialogue, as well as hybrid approaches that involve
        co-training for multiple tasks (e.g. response generation and rationale extraction), or the interfacing of
        symbolic methods with neural approaches. Finally we will explore regularisation strategies during generation
        that help optimise for combinations of the requirement criteria (e.g. privacy, text simplification). Model
        acceptability will be continuously evaluated by both humans in regular co-production workshops from Work Stream
        1 and according to the metrics and criteria identified in Work Stream 2.</p>
      <p><b>Outputs:</b> We will create: (a) <b>long-document summarisation prototypes for judicial reviews</b>, therapy
        sessions and mental health social media timelines and (b) <b>dialogue systems for AI-assisted self-management
          and
          legal advice</b>, developed to be situation-aware, according to methods focussing on privacy-safety,
        factuality and
        criteria from Work Streams 1 and 2.</p>
      <br>
      <div class="header" id="ws5b">
        <h5>Medical diagnostics and monitoring with multi-modal data</h5>
      </div>
      <p><b>Background & challenges:</b> LLM use in medicine raises concerns and challenges, reflective of the
        socio-technical limitations already discussed in use cases 1 and 2, and Work Streams 2 and 4. These are
        particularly prominent in medical diagnostics and monitoring because of the complexity of data sources and the
        potential impact associated with wrong or missed diagnoses and critical points. </p>
      <p><b>Foci & implementation:</b> We will focus on developing and testing the ability of LLMs to identify evidence
        for phenotypes characteristic of different conditions from heterogeneous multi-modal data, including NHS text
        records, medical imaging and user generated content (e.g. dialogues, devices) for people with cancer and
        dementia. To achieve this we will develop methods for co-training of medical code classification (e.g. ICD, DSM)
        and rationale generation and extraction, using multi-modal representations. We will also explore the current
        opportunities and limitations of Chain-of-Thought reasoning in generating explanations for medical diagnostics.
        We will develop methods for diagnosis and monitoring with longitudinal data and consider data augmentation
        methods for missing or sparse data. We will also explore the interaction between condition monitoring and
        dialogic responses in AI-supported self-management.</p>
      <p><b>Outputs:</b> We will create and evaluate <b>LLM-based methods for diagnostics and monitoring using
          heterogeneous
          and multi-modal data</b> in cancer, mental health and dementia. These will result in: (a) longitudinal
        diagnosis and
        monitoring and (b) self-management AI prototypes both justifying their output with evidence. The prototypes will
        help assess the likelihood of such models being rolled out in practice and their potential for enhancing medical
        care. </p>
    </div>
  </div>

  <br>

  <!-- WS5 -->
  <div class="container" data-aos="fade-up">

    <div class="icon-box1">
      <div id="ws5" class="icon"><i class="ri-open-arm-line"></i></div>
      <h4>Use Cases</h4>
      <p>Our current use cases and partners are in:
        <a href="#ws5a">(1)</a> Multi-modal medical diagnostics and monitoring for cancer and mental health;
        <a href="#ws5b">(2)</a> AI support for mental health covering summarisation & dialogue self-management
        interventions;
        <a href="#ws5c">(3)</a> AI legal support covering long-form summarisation and dialogue.
        We are considering new partners and use cases.
      </p>
      <br>
      <div class="header" id="ws5b">
        <h5>Use case 1: Multi-modal medical diagnostics and monitoring</h5>
      </div>
      <p>Patients in the UK have long waits for treatment, often until their condition deteriorates. Many conditions
        are multi-faceted with long-term changes not captured during single consultations/tests. The integration of
        longitudinal, heterogeneous, and multi-modal data, such as text (clinician notes, social media content, etc.),
        scan images, tabular data, phone mobility data, could offer a more complete view resulting in better diagnosis
        and monitoring of many conditions, including cancer and dementia.</p>
      <p><b>Key Challenges</b>: Data diversity & safeguarding; methods for longitudinal, multi-modal, heterogeneous
        and asynchronous data with sparse ground truth and missing values; evaluation metrics; interpretability and
        provenance of model outputs; the need for temporal reasoning and situational awareness, implications of the
        lack thereof. From an RAI perspective we must plan for unexpected consequences (e.g. balance between treatment
        availability and faster diagnosis). For monitoring, broader ethical and social challenges must be considered
        (e.g., perceptions of surveillance, trust, capacity).
      </p>
      <br>
      <div class="header" id="ws5b">
        <h5>Use Case 2: AI support for mental health</h5>
      </div>
      <p>(a) Summarisation & Monitoring: Standardised subjective measures are fundamental to mental health monitoring
        but have significant limitations: level self-awareness; willingness to complete questionnaires; limited choice
        of responses. Summaries that capture fluctuations in individuals' state-of-mind, based on heterogeneous
        sources, while emphasising key clinical concepts, can significantly assist in monitoring, prevention and early
        detection, augment clinician capacity, present alternatives to standard questionnaires and compensate for
        reduced access to mental health services. </p>
      <p><b>Key Challenges:</b> Fabricated or erroneous information (hallucinations); ethical challenges due to
        inappropriate generation or missing key events. From an RAI perspective, consideration of integration with
        current practice and NICE guidelines.</p>
      <p>(b) Self-management interventions: Such interventions have significant beneficial effects, even for people
        with severe mental illness, reducing symptoms and length of admission in inpatient units, improving social
        functioning & quality of life beyond end-of-treatment. AI-assisted models can support mental health clinicians
        and service users in developing shared strategies for self-management, allowing for personalisation in
        monitoring progress.</p>
      <p><b>Key Challenges:</b> Combining monitoring and dialogue technology; understanding how the technology meets
        patient & clinician needs; acceptability by patients (positively viewed vs fears of abandonment or exclusion);
        integration challenges with services care.</p>
      <br>
      <div class="header" id="ws5b">
        <h5>Use case 3: AI legal support</h5>
      </div>
      <p>The UK legal services market is the second largest in the world, employing >300,000 people and worth ~£43
        billion. However, the adoption of legaltech and specifically AI by UK law firms remains limited, hindered by
        trust. Yet interest in the application of AI, particularly language technology, has surged in recent years,
        including efforts to create specialised LLMs pre-trained on legal corpora. AI technology and generative LLMs
        have the potential to transform legal services, promising increased efficiency and robustness. Our use case
        will focus on: a) summarisation of legal documents used by judicial officials and b) dialogue systems for
        legal advice. Both are particularly relevant as judges are already allowed to summarise court cases using
        ChatGPT, and “DIY law” chatbots are being rolled out to give AI-powered legal answers. </p>
      <p><b>Key Challenges:</b> Appropriate generation (accurate and suitable information, without hallucination or
        unnecessary private information) and user-appropriate presentation of information (i.e. legal professionals vs
        general public). From an RAI perspective, safeguarding against erroneous or inappropriate responses and
        addressing liability issues in the latter case.</p>
    </div>
  </div>

</section>